{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import exists, join, basename, splitext\n",
    "\n",
    "import random\n",
    "import PIL\n",
    "import torchvision\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.set_grad_enabled(False)\n",
    "  \n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "plt.rcParams[\"axes.grid\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aidan\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\aidan\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MaskRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed in 1.696s\n",
      "Executed in 1.777s\n",
      "Executed in 1.666s\n",
      "Executed in 1.708s\n",
      "Executed in 1.793s\n",
      "Executed in 1.755s\n",
      "Executed in 1.744s\n",
      "Executed in 1.809s\n",
      "Executed in 1.778s\n",
      "Executed in 1.805s\n",
      "Executed in 1.857s\n",
      "Executed in 1.854s\n",
      "Executed in 1.807s\n",
      "Executed in 1.744s\n",
      "Executed in 1.748s\n",
      "Executed in 1.718s\n",
      "Executed in 1.779s\n",
      "Executed in 1.859s\n",
      "Executed in 1.744s\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import PIL.Image\n",
    "\n",
    "# Initialize webcam\n",
    "cam = cv2.VideoCapture(0)\n",
    "if not cam.isOpened(): \n",
    "    print(\"No camera detected!\")\n",
    "    exit()\n",
    "\n",
    "# COCO labels\n",
    "coco_names = [\n",
    "    'unlabeled', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', \n",
    "    'fire hydrant', 'street sign', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', \n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'hat', 'backpack', 'umbrella', 'shoe', 'eye glasses', 'handbag', 'tie', \n",
    "    'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', \n",
    "    'surfboard', 'tennis racket', 'bottle', 'plate', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', \n",
    "    'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', \n",
    "    'potted plant', 'bed', 'mirror', 'dining table', 'window', 'desk', 'toilet', 'door', 'tv', 'laptop', 'mouse', \n",
    "    'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'blender', 'book', \n",
    "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]\n",
    "\n",
    "# Random colors for each class\n",
    "colors = [[random.randint(0, 255) for _ in range(3)] for _ in coco_names]\n",
    "\n",
    "# Load pretrained Mask R-CNN\n",
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to capture image\")\n",
    "        break\n",
    "\n",
    "    # Convert frame to PIL Image and Tensor\n",
    "    image = PIL.Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    image_tensor = torchvision.transforms.functional.to_tensor(image).unsqueeze(0)\n",
    "\n",
    "    # Run inference\n",
    "    t = time.time()\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)[0]\n",
    "    print(f'Executed in {time.time() - t:.3f}s')\n",
    "\n",
    "    # Copy frame for drawing\n",
    "    result_image = frame.copy()\n",
    "    mask_overlay = np.zeros_like(frame, dtype=np.uint8)  # Create a blank mask\n",
    "\n",
    "    color = random.choice(colors)\n",
    "\n",
    "    # Draw bounding boxes, labels, and masks\n",
    "    for box, label, score, mask in zip(output['boxes'], output['labels'], output['scores'], output['masks']):\n",
    "        if score > 0.5:  # Confidence threshold\n",
    "            x1, y1, x2, y2 = map(int, box.tolist())\n",
    "\n",
    "            # Draw bounding box\n",
    "            thickness = 2\n",
    "            cv2.rectangle(result_image, (x1, y1), (x2, y2), color, thickness)\n",
    "\n",
    "            # Draw label\n",
    "            text = f\"{coco_names[label]}: {score:.1f}%\"\n",
    "            font_scale = 0.5\n",
    "            font_thickness = 1\n",
    "            text_size, _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, font_thickness)\n",
    "            text_x, text_y = x1, y1 - 5\n",
    "            cv2.rectangle(result_image, (text_x, text_y - text_size[1]), (text_x + text_size[0], text_y), color, -1)\n",
    "            cv2.putText(result_image, text, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, font_scale, (255, 255, 255), font_thickness)\n",
    "\n",
    "            # Process the mask\n",
    "            mask = mask.squeeze().cpu().numpy()  # Convert to numpy array\n",
    "            mask = (mask > 0.5).astype(np.uint8)  # Threshold the mask\n",
    "            mask_resized = cv2.resize(mask, (x2 - x1, y2 - y1))  # Resize mask\n",
    "            mask_resized = mask_resized[:, :, np.newaxis]  # Add channel dimension\n",
    "\n",
    "            # Color the mask\n",
    "            mask_color = np.array(color, dtype=np.uint8)    \n",
    "            mask_colored = mask_resized * mask_color  # Apply color\n",
    "\n",
    "            # Overlay mask onto the blank overlay image\n",
    "            mask_overlay[y1:y2, x1:x2] = np.maximum(mask_overlay[y1:y2, x1:x2], mask_colored)\n",
    "\n",
    "    # Blend mask overlay with the result image\n",
    "    alpha = 0.5  # Transparency factor\n",
    "    result_image = cv2.addWeighted(result_image, 1, mask_overlay, alpha, 0)\n",
    "\n",
    "    # Show result\n",
    "    cv2.imshow(\"Mask R-CNN - Real Time\", result_image)\n",
    "\n",
    "    # Exit on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Cleanup\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
